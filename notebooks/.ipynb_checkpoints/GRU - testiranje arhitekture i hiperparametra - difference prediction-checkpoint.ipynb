{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db05d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\mata2\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, RNN, LSTM, GRU, SpatialDropout1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebe99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e741663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d678cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\mata2\\Desktop\\master\\podaci\\0k\\X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cd7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4ae0b",
   "metadata": {},
   "source": [
    "Napravljen je pandas DataFrame gde su atributi zasebno pokrenute putanje, dok ce ciljna promenljiva da bude srednja vrednost svih putanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88795fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for file in dir:\n",
    "    file_names.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d37a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_train = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81c4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_val = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da659f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_names = file_names[:int(percent_train*len(file_names))]\n",
    "#test_file_names = file_names[int(percent_train*len(file_names))::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb96dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_command = np.asarray(pd.read_csv(r\"C:\\Users\\mata2\\Desktop\\master\\podaci\\xCmd.txt\",header=None).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d061a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = np.concatenate([pd.read_csv(path + \"/\" + file, header=None)[:61000].astype('int')\n",
    "                              for file in train_files_names], axis=0)\n",
    "test_ = np.concatenate([pd.read_csv(path + \"/\" + file, header=None)[:61000].astype('int')\n",
    "                              for file in test_file_names], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2fa3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = []\n",
    "with open(r\"C:\\Users\\mata2\\Desktop\\master\\Axis-projection-RNN\\odstupanja\\0k\\std_dev.txt\", 'r') as lines:\n",
    "    for line in lines:\n",
    "        std_dev.append(float(line.strip('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "519f6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_vrednost = []\n",
    "with open(r\"C:\\Users\\mata2\\Desktop\\master\\Axis-projection-RNN\\odstupanja\\0k\\sr_vrednost.txt\", 'r') as lines:\n",
    "    for line in lines:\n",
    "        sr_vrednost.append(float(line.strip('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11191964",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(train_files_names)):\n",
    "    exec(f\"razlika_trening_{j} = []\")\n",
    "    for i in range(61000):\n",
    "        exec(f\"razlika_trening_{j}.append(x_command[i] - train_[j*61000+i])\")\n",
    "    exec(f'razlika_trening_{j} = np.asarray(razlika_trening_{j})')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8786872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(test_file_names)):\n",
    "    exec(f\"razlika_test_{j} = []\")\n",
    "    for i in range(61000):\n",
    "        exec(f\"razlika_test_{j}.append(x_command[i] - test_[j*61000+i])\")\n",
    "    exec(f'razlika_test_{j} = np.asarray(razlika_test_{j})')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c0faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeseries_split(data, time_steps):\n",
    "\n",
    "  # Get the number of samples in the data\n",
    "  num_samples = len(data) - time_steps\n",
    "\n",
    "  # Create empty arrays to store features and target values\n",
    "  x_train = np.zeros((num_samples, time_steps, 1))\n",
    "  y_train = np.zeros((num_samples, 1))\n",
    "\n",
    "  # Loop through the data and create features and target values\n",
    "  for i in range(num_samples):\n",
    "    # Extract a slice of data for the current feature\n",
    "    x_train[i] = data[i:i+time_steps, :]\n",
    "\n",
    "    # The target value is the next value after the feature\n",
    "    y_train[i] = data[i+time_steps, 0]\n",
    "\n",
    "  return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb9a0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_kord = [float('inf') for x in range(61000)]\n",
    "max_kord = [float('-inf') for x in range(61000)]\n",
    "\n",
    "for i in range(61000):    \n",
    "    for j in range(len(train_files_names)):\n",
    "        exec(f'current_file = razlika_trening_{j}')\n",
    "        if current_file[i][0] < min_kord[i]:\n",
    "            min_kord[i] = current_file[i][0]\n",
    "        if current_file[i][0] > max_kord[i]:\n",
    "            max_kord[i] = current_file[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7aeaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 8\n",
    "input_shape = (time_steps,1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a9edfc5",
   "metadata": {},
   "source": [
    "for i in range(len(train_files_names)):\n",
    "    exec(f'train_X_{i}, train_y_{i} = create_timeseries_split(razlika_trening_{i}, time_steps)')\n",
    "\n",
    "for i in range(len(test_file_names)):\n",
    "    exec(f'test_X_{i}, test_y_{i} = create_timeseries_split(razlika_test_{i}, time_steps)')\n",
    "    \n",
    "train_X = np.asarray(train_X_0)\n",
    "for i in range(1,len(train_files_names)):\n",
    "    exec(f'train_X = np.append(train_X, train_X_{i}, axis=0)')    \n",
    "    \n",
    "train_y = np.asarray(train_y_0)\n",
    "for i in range(1,len(train_files_names)):\n",
    "    exec(f'train_y = np.append(train_y, train_y_{i}, axis=0)')    \n",
    "    \n",
    "test_X = np.asarray(test_X_0)\n",
    "for i in range(1,len(test_file_names)):\n",
    "    exec(f'test_X = np.append(test_X, test_X_{i}, axis=0)')    \n",
    "    \n",
    "test_y = np.asarray(test_y_0)\n",
    "for i in range(1,len(test_file_names)):\n",
    "    exec(f'test_y = np.append(test_y, test_y_{i}, axis=0)')    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a6fd572",
   "metadata": {},
   "source": [
    "keras.utils.set_random_seed(7)\n",
    "optimizerMSE = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "modelMSE = Sequential()\n",
    "modelMSE.add(GRU(units = 16, input_shape = input_shape, activation = keras.layers.LeakyReLU()))\n",
    "modelMSE.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "modelMSE.add(Dense(units = 16, activation=keras.layers.LeakyReLU()))\n",
    "#modelMSE.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "modelMSE.add(Dense(units = 1, activation=keras.layers.LeakyReLU(), bias_initializer='zeros', kernel_initializer='normal'))\n",
    "modelMSE.compile(loss='mean_squared_error', optimizer = optimizerMSE)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb81ad1b",
   "metadata": {},
   "source": [
    "historyMSE = modelMSE.fit(train_X, train_y, epochs=10, batch_size = 64, verbose = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "477af0ba",
   "metadata": {},
   "source": [
    "train_scores = modelMSE.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfa70718",
   "metadata": {},
   "source": [
    "train_predict_MSE = modelMSE.predict(train_X)\n",
    "test_predict_MSE = modelMSE.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42053f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_list = [8,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bf1540a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Users\\mata2\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "22872/22872 - 68s - loss: 4.9113 - 68s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "22872/22872 - 65s - loss: 2.2804 - 65s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "22872/22872 - 55s - loss: 2.1692 - 55s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "22872/22872 - 55s - loss: 2.1119 - 55s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "22872/22872 - 54s - loss: 2.0642 - 54s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "22872/22872 - 51s - loss: 2.0256 - 51s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "22872/22872 - 52s - loss: 1.9920 - 52s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "22872/22872 - 55s - loss: 1.9628 - 55s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "22872/22872 - 54s - loss: 1.9397 - 54s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "22872/22872 - 54s - loss: 1.9181 - 54s/epoch - 2ms/step\n",
      "11436/11436 [==============================] - 17s 1ms/step\n",
      "best_skor 312152 best_step 8\n",
      "Epoch 1/10\n",
      "22872/22872 - 83s - loss: 5.2565 - 83s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "22872/22872 - 72s - loss: 2.3516 - 72s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "22872/22872 - 86s - loss: 2.2094 - 86s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "22872/22872 - 90s - loss: 2.1304 - 90s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "22872/22872 - 88s - loss: 2.0707 - 88s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "22872/22872 - 89s - loss: 2.0262 - 89s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "22872/22872 - 89s - loss: 1.9865 - 89s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "22872/22872 - 90s - loss: 1.9539 - 90s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "22872/22872 - 88s - loss: 1.9297 - 88s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "22872/22872 - 88s - loss: 1.9070 - 88s/epoch - 4ms/step\n",
      "11436/11436 [==============================] - 21s 2ms/step\n",
      "best_skor 312152\n",
      "best_step 8\n"
     ]
    }
   ],
   "source": [
    "best_step = None\n",
    "best_skor = float('-inf')\n",
    "\n",
    "for step in time_steps_list:\n",
    "    for i in range(len(train_files_names)):\n",
    "        exec(f'train_X_{i}, train_y_{i} = create_timeseries_split(razlika_trening_{i}, step)')\n",
    "    for i in range(len(test_file_names)):\n",
    "        exec(f'test_X_{i}, test_y_{i} = create_timeseries_split(razlika_test_{i}, step)')\n",
    "        \n",
    "    train_X = np.asarray(train_X_0)\n",
    "    for i in range(1,len(train_files_names)):\n",
    "        exec(f'train_X = np.append(train_X, train_X_{i}, axis=0)')\n",
    "        \n",
    "    train_y = np.asarray(train_y_0)\n",
    "    for i in range(1,len(train_files_names)):\n",
    "        exec(f'train_y = np.append(train_y, train_y_{i}, axis=0)') \n",
    "    \n",
    "    test_X = np.asarray(test_X_0)\n",
    "    for i in range(1,len(test_file_names)):\n",
    "        exec(f'test_X = np.append(test_X, test_X_{i}, axis=0)')\n",
    "    \n",
    "    test_y = np.asarray(test_y_0)\n",
    "    for i in range(1,len(test_file_names)):\n",
    "        exec(f'test_y = np.append(test_y, test_y_{i}, axis=0)')\n",
    "\n",
    "    input_shape = (step,1)\n",
    "    \n",
    "    keras.utils.set_random_seed(7)\n",
    "    optimizerMSE = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    modelMSE = Sequential()\n",
    "    modelMSE.add(GRU(units = 16, input_shape = input_shape, activation = keras.layers.LeakyReLU()))\n",
    "    modelMSE.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "    modelMSE.add(Dense(units = 16, activation=keras.layers.LeakyReLU()))\n",
    "    #modelMSE.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "    modelMSE.add(Dense(units = 1, activation=keras.layers.LeakyReLU(), bias_initializer='zeros', kernel_initializer='normal'))\n",
    "    modelMSE.compile(loss='mean_squared_error', optimizer = optimizerMSE)\n",
    "    \n",
    "    historyMSE = modelMSE.fit(train_X, train_y, epochs=10, batch_size = 64, verbose = 2, shuffle = False)\n",
    "    #train_scores = modelMSE.evaluate(test_X, test_y)\n",
    "    test_predict_MSE = modelMSE.predict(test_X)\n",
    "    \n",
    "    uspeli_minmax = 0\n",
    "    for j in range(len(test_file_names)):\n",
    "        for i in range(int(len(test_predict_MSE)/len(test_file_names))):\n",
    "            if min_kord[i+time_steps] <= test_predict_MSE[int(len(test_predict_MSE)/len(test_file_names))*j + i] <= max_kord[i+time_steps]:\n",
    "                uspeli_minmax += 1\n",
    "    \n",
    "    if uspeli_minmax > best_skor:\n",
    "        best_skor = uspeli_minmax\n",
    "        best_step = step\n",
    "        \n",
    "        print('best_skor', best_skor, 'best_step',best_step)\n",
    "\n",
    "print('best_skor', best_skor)\n",
    "print('best_step', best_step)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d48507b",
   "metadata": {},
   "source": [
    "best steps [8 > 5 >~10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be3282eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 8\n",
    "input_shape = (time_steps,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a87ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_files_names)):\n",
    "    exec(f'train_X_{i}, train_y_{i} = create_timeseries_split(razlika_trening_{i}, time_steps)')\n",
    "for i in range(len(test_file_names)):\n",
    "    exec(f'test_X_{i}, test_y_{i} = create_timeseries_split(razlika_test_{i}, time_steps)')\n",
    "\n",
    "train_X = np.asarray(train_X_0)\n",
    "for i in range(1,len(train_files_names)):\n",
    "    exec(f'train_X = np.append(train_X, train_X_{i}, axis=0)')\n",
    "\n",
    "train_y = np.asarray(train_y_0)\n",
    "for i in range(1,len(train_files_names)):\n",
    "    exec(f'train_y = np.append(train_y, train_y_{i}, axis=0)') \n",
    "\n",
    "test_X = np.asarray(test_X_0)\n",
    "for i in range(1,len(test_file_names)):\n",
    "    exec(f'test_X = np.append(test_X, test_X_{i}, axis=0)')\n",
    "\n",
    "test_y = np.asarray(test_y_0)\n",
    "for i in range(1,len(test_file_names)):\n",
    "    exec(f'test_y = np.append(test_y, test_y_{i}, axis=0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d992731",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = train_X[-int(len(train_X)*0.2)-1:-1]\n",
    "val_y = train_y[-int(len(train_X)*0.2)-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7fbeddc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292761"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eedcf6ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X[:val_x.shape[0]]\n",
    "train_y = train_y[:val_y.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a86c9",
   "metadata": {},
   "source": [
    "# TESTIRANJE ARHITEKTURE i LR/BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8662585",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_unit = [8,16]\n",
    "unit1 = [8,16]\n",
    "unit2 = [8,16]\n",
    "LR = [0.01,0.001]\n",
    "batch_size = [32,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a33e5a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18298/18298 - 60s - loss: 9.9657 - 60s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 54s - loss: 2.7851 - 54s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 52s - loss: 2.5467 - 52s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 51s - loss: 2.4423 - 51s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 50s - loss: 2.3723 - 50s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 51s - loss: 2.3055 - 51s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 53s - loss: 2.2565 - 53s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 53s - loss: 2.2270 - 53s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 53s - loss: 2.2036 - 53s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 53s - loss: 2.1839 - 53s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 14s 1ms/step\n",
      "best_skor:  169032 best_gru:  8 best_unit1:  8 best_unit2:  8\n",
      "Epoch 1/10\n",
      "18298/18298 - 58s - loss: 10.4502 - 58s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 55s - loss: 2.9156 - 55s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 54s - loss: 2.4645 - 54s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 54s - loss: 2.3429 - 54s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 54s - loss: 2.2860 - 54s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 54s - loss: 2.2469 - 54s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 54s - loss: 2.2123 - 54s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 54s - loss: 2.1815 - 54s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 54s - loss: 2.1516 - 54s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 54s - loss: 2.1242 - 54s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 14s 1ms/step\n",
      "best_skor:  171848 best_gru:  8 best_unit1:  8 best_unit2:  16\n",
      "Epoch 1/10\n",
      "18298/18298 - 57s - loss: 10.3603 - 57s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 54s - loss: 2.8432 - 54s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 54s - loss: 2.5474 - 54s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 54s - loss: 2.4230 - 54s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 55s - loss: 2.3626 - 55s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 55s - loss: 2.3218 - 55s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 56s - loss: 2.2842 - 56s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 65s - loss: 2.2501 - 65s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 72s - loss: 2.2194 - 72s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 70s - loss: 2.1917 - 70s/epoch - 4ms/step\n",
      "9149/9149 [==============================] - 19s 2ms/step\n",
      "best_skor:  172145 best_gru:  8 best_unit1:  16 best_unit2:  8\n",
      "Epoch 1/10\n",
      "18298/18298 - 74s - loss: 8.8625 - 74s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 71s - loss: 2.6600 - 71s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 70s - loss: 2.3781 - 70s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 71s - loss: 2.3057 - 71s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 71s - loss: 2.2631 - 71s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 70s - loss: 2.2329 - 70s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 70s - loss: 2.2094 - 70s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 68s - loss: 2.1878 - 68s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 67s - loss: 2.1689 - 67s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 67s - loss: 2.1530 - 67s/epoch - 4ms/step\n",
      "9149/9149 [==============================] - 20s 2ms/step\n",
      "Epoch 1/10\n",
      "18298/18298 - 58s - loss: 8.5863 - 58s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 56s - loss: 2.7714 - 56s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 57s - loss: 2.4156 - 57s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 62s - loss: 2.3103 - 62s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 65s - loss: 2.2570 - 65s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 55s - loss: 2.2146 - 55s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 54s - loss: 2.1755 - 54s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 55s - loss: 2.1437 - 55s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 55s - loss: 2.1156 - 55s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 55s - loss: 2.0904 - 55s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 14s 2ms/step\n",
      "best_skor:  173067 best_gru:  16 best_unit1:  8 best_unit2:  8\n",
      "Epoch 1/10\n",
      "18298/18298 - 58s - loss: 8.7139 - 58s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 56s - loss: 2.4879 - 56s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 56s - loss: 2.2358 - 56s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 55s - loss: 2.1495 - 55s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 55s - loss: 2.1012 - 55s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 56s - loss: 2.0679 - 56s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 55s - loss: 2.0434 - 55s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 55s - loss: 2.0236 - 55s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 55s - loss: 2.0052 - 55s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 55s - loss: 1.9888 - 55s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 14s 2ms/step\n",
      "Epoch 1/10\n",
      "18298/18298 - 59s - loss: 7.7718 - 59s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 58s - loss: 2.5017 - 58s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 58s - loss: 2.3046 - 58s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 68s - loss: 2.2124 - 68s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 62s - loss: 2.1579 - 62s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 66s - loss: 2.1233 - 66s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 59s - loss: 2.0974 - 59s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 63s - loss: 2.0758 - 63s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 67s - loss: 2.0579 - 67s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 67s - loss: 2.0426 - 67s/epoch - 4ms/step\n",
      "9149/9149 [==============================] - 17s 2ms/step\n",
      "Epoch 1/10\n",
      "18298/18298 - 68s - loss: 6.8699 - 68s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 65s - loss: 2.4636 - 65s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 63s - loss: 2.3108 - 63s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 59s - loss: 2.2286 - 59s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 56s - loss: 2.1742 - 56s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 51s - loss: 2.1357 - 51s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 52s - loss: 2.1044 - 52s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 56s - loss: 2.0796 - 56s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 56s - loss: 2.0586 - 56s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 56s - loss: 2.0404 - 56s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 14s 2ms/step\n",
      "best_skor:  173067\n",
      "best_gru:  16\n",
      "best_unit1:  8\n",
      "best_unit2:  8\n"
     ]
    }
   ],
   "source": [
    "best_skor = float('-inf')\n",
    "best_gru = None\n",
    "best_unit1 = None\n",
    "best_unit2 = None\n",
    "#best_batch = None\n",
    "#best_LR = None\n",
    "\n",
    "for gru in gru_unit:\n",
    "    for uni1 in unit1:\n",
    "        for uni2 in unit2:\n",
    "            #for rate in LR:\n",
    "            #for batch in batch_size:\n",
    "\n",
    "            keras.utils.set_random_seed(7)\n",
    "            optimizerMSE = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(GRU(units = gru, input_shape = input_shape, activation = keras.layers.LeakyReLU()))\n",
    "            model.add(Dense(units = uni1, activation=keras.layers.LeakyReLU()))\n",
    "            model.add(Dense(units = uni2, activation=keras.layers.LeakyReLU()))\n",
    "            model.add(Dense(units = 1, activation=keras.layers.LeakyReLU(), bias_initializer='zeros', kernel_initializer='normal'))\n",
    "            model.compile(loss='mean_squared_error', optimizer = optimizerMSE)\n",
    "\n",
    "            model.fit(train_X, train_y, epochs = 10, batch_size = 64, verbose = 2, shuffle = False)\n",
    "            test_predict_MSE = model.predict(val_x)\n",
    "            \n",
    "#             uspeli_minmax = 0\n",
    "#             for j in range(len(test_file_names)):\n",
    "#                 for i in range(int(len(test_predict_MSE)/len(test_file_names))):\n",
    "#                     if min_kord[i+time_steps] <= test_predict_MSE[int(len(test_predict_MSE)/len(test_file_names))*j + i] <= max_kord[i+time_steps]:\n",
    "#                         uspeli_minmax += 1\n",
    "            \n",
    "            i = 8\n",
    "            uspeli_minmax = 0\n",
    "            for j in range(len(test_predict_MSE)):\n",
    "                if i%61000 == 0:\n",
    "                    i = 8\n",
    "                if min_kord[i] <= test_predict_MSE[j] <= max_kord[i]:\n",
    "                    uspeli_minmax += 1\n",
    "                i += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            if uspeli_minmax > best_skor:\n",
    "                best_skor = uspeli_minmax\n",
    "                best_gru = gru\n",
    "                best_unit1 = uni1\n",
    "                best_unit2 = uni2\n",
    "                #best_LR = rate\n",
    "                #best_batch = batch\n",
    "\n",
    "                print('best_skor: ', best_skor,'best_gru: ', best_gru,'best_unit1: ', best_unit1,'best_unit2: ', best_unit2)\n",
    "print('best_skor: ', best_skor)                    \n",
    "print('best_gru: ', best_gru)\n",
    "print('best_unit1: ', best_unit1)\n",
    "print('best_unit2: ', best_unit2)\n",
    "#print('best learning rate: ', best_LR)    \n",
    "#print('best batch size: ', best_batch)    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94e914a2",
   "metadata": {},
   "source": [
    "MAE:\n",
    "/\n",
    "MSE:\n",
    "best_skor:  1.8558906316757202\n",
    "best_gru:  16\n",
    "best_unit1:  8\n",
    "best_unit2:  16\n",
    "\n",
    "best_skor:  331216 (koriscen test skup)\n",
    "best_gru:  8\n",
    "best_unit1:  8\n",
    "best_unit2:  8\n",
    "\n",
    "best_skor:  173067/292761 (koriscen validacioni skup od 20% trening skupa)\n",
    "best_gru:  16\n",
    "best_unit1:  8\n",
    "best_unit2:  8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0c015429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292761"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_predict_MSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eb6d621",
   "metadata": {},
   "source": [
    "LR = [0.01,0.001,0.0001,0.00001]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "075db969",
   "metadata": {},
   "source": [
    "best_skor_LR = float('inf')\n",
    "best_LR = None\n",
    "\n",
    "for rate in LR:\n",
    "    keras.utils.set_random_seed(7)\n",
    "    optimizerMSE = keras.optimizers.Adam(learning_rate=rate)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(units = 16, input_shape = input_shape, activation = keras.layers.LeakyReLU()))\n",
    "    model.add(Dense(units = 16, activation=keras.layers.LeakyReLU()))\n",
    "    model.add(Dense(units = 16, activation=keras.layers.LeakyReLU()))\n",
    "    model.add(Dense(units = 1, activation=keras.layers.LeakyReLU(), bias_initializer='zeros', kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer = optimizerMSE)\n",
    "    \n",
    "    model.fit(train_X, train_y, epochs=10, batch_size = 64, verbose = 2, shuffle = False)\n",
    "    train_scores = model.evaluate(test_X, test_y)\n",
    "    \n",
    "    if train_scores < best_skor_LR:\n",
    "        best_skor_LR = train_scores\n",
    "        best_LR = rate\n",
    "print('best_skor: ', best_skor_LR)\n",
    "print('best learning rate: ', best_LR)    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e2071e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "06161cd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90b495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b696ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b3f4c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [64,128,256,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "357732c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18298/18298 - 65s - loss: 5.8070 - 65s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "18298/18298 - 55s - loss: 2.5542 - 55s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "18298/18298 - 55s - loss: 2.3393 - 55s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "18298/18298 - 55s - loss: 2.2545 - 55s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "18298/18298 - 55s - loss: 2.1963 - 55s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "18298/18298 - 54s - loss: 2.1544 - 54s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "18298/18298 - 54s - loss: 2.1215 - 54s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "18298/18298 - 54s - loss: 2.0970 - 54s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "18298/18298 - 54s - loss: 2.0770 - 54s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "18298/18298 - 54s - loss: 2.0596 - 54s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 14s 2ms/step\n",
      "Epoch 1/10\n",
      "9149/9149 - 33s - loss: 7.8209 - 33s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "9149/9149 - 30s - loss: 2.8426 - 30s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "9149/9149 - 30s - loss: 2.4959 - 30s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "9149/9149 - 30s - loss: 2.3822 - 30s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "9149/9149 - 30s - loss: 2.3047 - 30s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "9149/9149 - 30s - loss: 2.2506 - 30s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "9149/9149 - 30s - loss: 2.2128 - 30s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "9149/9149 - 30s - loss: 2.1834 - 30s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "9149/9149 - 30s - loss: 2.1585 - 30s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "9149/9149 - 30s - loss: 2.1368 - 30s/epoch - 3ms/step\n",
      "9149/9149 [==============================] - 15s 2ms/step\n",
      "Epoch 1/10\n",
      "4575/4575 - 19s - loss: 11.0351 - 19s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "4575/4575 - 17s - loss: 3.5444 - 17s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "4575/4575 - 17s - loss: 2.8320 - 17s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "4575/4575 - 17s - loss: 2.5183 - 17s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "4575/4575 - 17s - loss: 2.3824 - 17s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "4575/4575 - 17s - loss: 2.3128 - 17s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "4575/4575 - 17s - loss: 2.2603 - 17s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "4575/4575 - 17s - loss: 2.2181 - 17s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "4575/4575 - 17s - loss: 2.1827 - 17s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "4575/4575 - 17s - loss: 2.1582 - 17s/epoch - 4ms/step\n",
      "9149/9149 [==============================] - 14s 2ms/step\n",
      "Epoch 1/10\n",
      "2288/2288 - 13s - loss: 15.8567 - 13s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "2288/2288 - 11s - loss: 4.8173 - 11s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "2288/2288 - 11s - loss: 3.3667 - 11s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "2288/2288 - 11s - loss: 2.8799 - 11s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "2288/2288 - 11s - loss: 2.6397 - 11s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "2288/2288 - 11s - loss: 2.5110 - 11s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "2288/2288 - 11s - loss: 2.4348 - 11s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "2288/2288 - 11s - loss: 2.3904 - 11s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "2288/2288 - 11s - loss: 2.3551 - 11s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "2288/2288 - 11s - loss: 2.3286 - 11s/epoch - 5ms/step\n",
      "9149/9149 [==============================] - 15s 2ms/step\n",
      "best_skor:  174939\n",
      "best batch size:  128\n"
     ]
    }
   ],
   "source": [
    "best_skor_batch = float('-inf')\n",
    "best_batch = None\n",
    "\n",
    "for batch in batch_size:\n",
    "    optimizerMSE = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    keras.utils.set_random_seed(7)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(units = 16, input_shape = input_shape, activation = keras.layers.LeakyReLU()))\n",
    "    model.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "    model.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "    model.add(Dense(units = 1, activation=keras.layers.LeakyReLU(), bias_initializer='zeros', kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer = optimizerMSE)\n",
    "    \n",
    "    model.fit(train_X, train_y, epochs=10, batch_size = batch, verbose = 2)\n",
    "    #train_scores = model.evaluate(test_X, test_y)\n",
    "    test_predict_MSE = model.predict(val_x)\n",
    "    i = 8\n",
    "    uspeli_minmax = 0\n",
    "    for j in range(len(test_predict_MSE)):\n",
    "        if i%61000 == 0:\n",
    "            i = 8\n",
    "        if min_kord[i] <= test_predict_MSE[j] <= max_kord[i]:\n",
    "            uspeli_minmax += 1\n",
    "        i += 1\n",
    "    \n",
    "    if uspeli_minmax > best_skor_batch:\n",
    "        best_skor_batch = uspeli_minmax\n",
    "        best_batch = batch\n",
    "print('best_skor: ', best_skor_batch)\n",
    "print('best batch size: ', best_batch)    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fb408a7",
   "metadata": {},
   "source": [
    "MSE\n",
    "best_skor:  1.1485064029693604\n",
    "best batch size:  64\n",
    "\n",
    "best_skor:  174939 (minmax)\n",
    "best batch size:  128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ceb74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(7)\n",
    "optimizerMSE = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "modelMSE = Sequential()\n",
    "modelMSE.add(GRU(units = 16, input_shape = input_shape, activation = keras.layers.LeakyReLU()))\n",
    "modelMSE.add(Dense(units = 16, activation=keras.layers.LeakyReLU()))\n",
    "modelMSE.add(Dense(units = 16, activation=keras.layers.LeakyReLU()))\n",
    "#modelMSE.add(Dense(units = 8, activation=keras.layers.LeakyReLU()))\n",
    "modelMSE.add(Dense(units = 1, activation=keras.layers.LeakyReLU(), bias_initializer='zeros', kernel_initializer='normal'))\n",
    "modelMSE.compile(loss='mean_squared_error', optimizer = optimizerMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68b8acb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelMAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m historyMAE \u001b[38;5;241m=\u001b[39m modelMAE\u001b[38;5;241m.\u001b[39mfit(train_X, train_y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelMAE' is not defined"
     ]
    }
   ],
   "source": [
    "historyMAE = modelMAE.fit(train_X, train_y, epochs=10, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8282cb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "894/894 - 9s - loss: 7173579776.0000 - 9s/epoch - 10ms/step\n",
      "Epoch 2/10\n",
      "894/894 - 2s - loss: 48031884.0000 - 2s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "894/894 - 2s - loss: 1464.7377 - 2s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "894/894 - 2s - loss: 1464.3621 - 2s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "894/894 - 2s - loss: 1464.1544 - 2s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "894/894 - 2s - loss: 1464.0243 - 2s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "894/894 - 2s - loss: 1463.9316 - 2s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "894/894 - 2s - loss: 1463.8832 - 2s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "894/894 - 2s - loss: 1463.6785 - 2s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "894/894 - 2s - loss: 1463.3251 - 2s/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "historyMSE = modelMSE.fit(train_X, train_y, epochs=10, batch_size = 1024, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "454b9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28594/28594 [==============================] - 37s 1ms/step - loss: 1463.4017\n"
     ]
    }
   ],
   "source": [
    "#train_scores_MAE = modelMAE.evaluate(test_X, test_y)\n",
    "train_scores_MSE = modelMSE.evaluate(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ce84951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28594/28594 [==============================] - 36s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predict_MSE = modelMSE.predict(test_X)\n",
    "#test_predict_MAE = modelMAE.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321747b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc50b1d",
   "metadata": {},
   "source": [
    "# TESTIRANJE SA VISE INSTANCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db00ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
